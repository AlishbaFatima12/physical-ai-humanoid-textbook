"use strict";(globalThis.webpackChunkphysical_ai_humanoid_textbook=globalThis.webpackChunkphysical_ai_humanoid_textbook||[]).push([[4833],{7315:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-8-vla-humanoid","title":"Chapter 8: Voice-Language-Action for Humanoids","description":"Build multimodal humanoid interfaces with voice, language, and action. Orchestrate skills (Ch. 6-7) using LLMs. Layer 4 - Spec-Driven Integration.\\n","source":"@site/docs/008-chapter-8-vla-humanoid.md","sourceDirName":".","slug":"/chapter-8-vla-humanoid","permalink":"/physical-ai-humanoid-textbook/chapter-8-vla-humanoid","draft":false,"unlisted":false,"editUrl":"https://github.com/AlishbaFatima12/physical-ai-humanoid-textbook/edit/main/docs/docs/008-chapter-8-vla-humanoid.md","tags":[{"inline":true,"label":"layer4","permalink":"/physical-ai-humanoid-textbook/tags/layer-4"},{"inline":true,"label":"vla","permalink":"/physical-ai-humanoid-textbook/tags/vla"},{"inline":true,"label":"orchestration","permalink":"/physical-ai-humanoid-textbook/tags/orchestration"}],"version":"current","lastUpdatedBy":"AlishbaFatima12","lastUpdatedAt":1764391712000,"sidebarPosition":8,"frontMatter":{"id":"chapter-8-vla-humanoid","title":"Chapter 8: Voice-Language-Action for Humanoids","sidebar_position":8,"description":"Build multimodal humanoid interfaces with voice, language, and action. Orchestrate skills (Ch. 6-7) using LLMs. Layer 4 - Spec-Driven Integration.\\n","keywords":["VLA","LLM","multimodal","skill orchestration","spec-driven"],"tags":["layer4","vla","orchestration"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 7: Path Planning & Reinforcement Learning","permalink":"/physical-ai-humanoid-textbook/chapter-7-path-planning-rl"},"next":{"title":"Chapter 9: Capstone Project","permalink":"/physical-ai-humanoid-textbook/chapter-9-capstone"}}');var s=r(4848),t=r(8453);const l={id:"chapter-8-vla-humanoid",title:"Chapter 8: Voice-Language-Action for Humanoids",sidebar_position:8,description:"Build multimodal humanoid interfaces with voice, language, and action. Orchestrate skills (Ch. 6-7) using LLMs. Layer 4 - Spec-Driven Integration.\n",keywords:["VLA","LLM","multimodal","skill orchestration","spec-driven"],tags:["layer4","vla","orchestration"]},a="Chapter 8: Voice-Language-Action for Humanoids",o={},c=[{value:"Overview",id:"overview",level:2},{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Component 1: Speech Recognition",id:"component-1-speech-recognition",level:2},{value:"Using Whisper (OpenAI)",id:"using-whisper-openai",level:3},{value:"Component 2: LLM Planner",id:"component-2-llm-planner",level:2},{value:"Task Decomposition with GPT-4",id:"task-decomposition-with-gpt-4",level:3},{value:"Component 3: Skill Orchestrator",id:"component-3-skill-orchestrator",level:2},{value:"End-to-End Example",id:"end-to-end-example",level:2},{value:"User Command: &quot;Go to the kitchen and find a cup&quot;",id:"user-command-go-to-the-kitchen-and-find-a-cup",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Integrate Whisper Speech Recognition (Easy)",id:"exercise-1-integrate-whisper-speech-recognition-easy",level:3},{value:"Exercise 2: Build LLM Planner (Medium)",id:"exercise-2-build-llm-planner-medium",level:3},{value:"Exercise 3: End-to-End VLA Pipeline (Hard)",id:"exercise-3-end-to-end-vla-pipeline-hard",level:3},{value:"Spec-Driven Development \u2b50",id:"spec-driven-development-",level:2},{value:"Task Specification Format",id:"task-specification-format",level:3},{value:"Assessment Questions",id:"assessment-questions",level:2},{value:"Self-Check",id:"self-check",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-8-voice-language-action-for-humanoids",children:"Chapter 8: Voice-Language-Action for Humanoids"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Learning Objectives:"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate speech recognition for voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Use LLMs (GPT-4, Claude) to interpret natural language instructions"}),"\n",(0,s.jsx)(n.li,{children:"Orchestrate perception and navigation skills (Ch. 6-7) via LLM reasoning"}),"\n",(0,s.jsx)(n.li,{children:"Build end-to-end VLA pipeline (voice \u2192 plan \u2192 execute)"}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement spec-driven integration"})," \u2b50 (Layer 4)"]}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"Prerequisites",type:"info",children:(0,s.jsxs)(n.p,{children:["Complete ",(0,s.jsx)(n.a,{href:"./chapter-7-path-planning-rl",children:"Chapter 7"})," - Navigation and control skills"]})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Estimated Duration"}),": 10 hours (lecture + lab)"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Layer Enforcement"}),": ",(0,s.jsx)(n.strong,{children:"Layer 4 (Spec-Driven Integration)"}),". Build orchestrated systems with:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Clear API contracts between skills"}),"\n",(0,s.jsx)(n.li,{children:"LLM-based reasoning for multi-step tasks"}),"\n",(0,s.jsx)(n.li,{children:"Formal task specifications"}),"\n",(0,s.jsx)(n.li,{children:"\u226570% capstone project success rate"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Voice-Language-Action (VLA)"})," integrates:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice"}),": Speech recognition (user commands)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language"}),": LLM reasoning (task decomposition)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action"}),": Skill execution (navigation, manipulation)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'User: "Go to the kitchen and bring me a cup"\r\n\r\nVoice \u2192 "Go to the kitchen and bring me a cup"\r\nLanguage (LLM) \u2192\r\n  1. Navigate to kitchen\r\n  2. Detect cup\r\n  3. Grasp cup\r\n  4. Navigate to user\r\n  5. Hand over cup\r\n\r\nAction \u2192 Execute skills 1-5\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\r\n    A[Voice Input] --\x3e B[Speech Recognition]\r\n    B --\x3e C[Text Command]\r\n    C --\x3e D[LLM Planner]\r\n    D --\x3e E[Task Plan]\r\n    E --\x3e F[Skill Orchestrator]\r\n    F --\x3e G1[Navigation Skill]\r\n    F --\x3e G2[Perception Skill]\r\n    F --\x3e G3[Manipulation Skill]\r\n    G1 --\x3e H[Robot Actions]\r\n    G2 --\x3e H\r\n    G3 --\x3e H\r\n\r\n    style D fill:#FF6B6B\r\n    style F fill:#4ECDC4\r\n    style H fill:#95E1D3"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"component-1-speech-recognition",children:"Component 1: Speech Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"using-whisper-openai",children:"Using Whisper (OpenAI)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="vla_system/speech_recognition_node.py"',children:'"""\r\nPurpose: Convert voice to text using Whisper\r\nPrerequisites: openai-whisper package\r\nExpected Output: Transcribed text published to /voice_command\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\nimport pyaudio\r\nimport wave\r\n\r\nclass SpeechRecognitionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'speech_recognition\')\r\n        self.publisher = self.create_publisher(String, \'/voice_command\', 10)\r\n\r\n        # Load Whisper model\r\n        self.model = whisper.load_model("base")  # Options: tiny, base, small, medium, large\r\n\r\n        self.get_logger().info("Speech recognition ready. Say \'Hey robot\'...")\r\n\r\n    def listen_and_transcribe(self):\r\n        """Record audio and transcribe using Whisper."""\r\n\r\n        # Record audio (3 seconds)\r\n        audio_data = self.record_audio(duration=3)\r\n\r\n        # Transcribe\r\n        result = self.model.transcribe(audio_data)\r\n        text = result["text"]\r\n\r\n        self.get_logger().info(f"Recognized: {text}")\r\n\r\n        # Publish to ROS 2\r\n        msg = String()\r\n        msg.data = text\r\n        self.publisher.publish(msg)\r\n\r\n    def record_audio(self, duration=3):\r\n        """Record audio from microphone."""\r\n        CHUNK = 1024\r\n        FORMAT = pyaudio.paInt16\r\n        CHANNELS = 1\r\n        RATE = 16000\r\n\r\n        p = pyaudio.PyAudio()\r\n        stream = p.open(format=FORMAT, channels=CHANNELS,\r\n                        rate=RATE, input=True, frames_per_buffer=CHUNK)\r\n\r\n        frames = []\r\n        for _ in range(0, int(RATE / CHUNK * duration)):\r\n            data = stream.read(CHUNK)\r\n            frames.append(data)\r\n\r\n        stream.stop_stream()\r\n        stream.close()\r\n        p.terminate()\r\n\r\n        return b\'\'.join(frames)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SpeechRecognitionNode()\r\n\r\n    while rclpy.ok():\r\n        node.listen_and_transcribe()\r\n\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"component-2-llm-planner",children:"Component 2: LLM Planner"}),"\n",(0,s.jsx)(n.h3,{id:"task-decomposition-with-gpt-4",children:"Task Decomposition with GPT-4"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="vla_system/llm_planner_node.py"',children:'"""\r\nPurpose: Decompose natural language commands into skill sequences\r\nPrerequisites: OpenAI API key, skill library\r\nExpected Output: JSON task plan\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport openai\r\nimport json\r\n\r\nclass LLMPlannerNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'llm_planner\')\r\n\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/voice_command\',\r\n            self.plan_callback,\r\n            10\r\n        )\r\n\r\n        self.publisher = self.create_publisher(String, \'/task_plan\', 10)\r\n\r\n        # OpenAI API\r\n        openai.api_key = "YOUR_API_KEY"  # Use environment variable in production\r\n\r\n        # Skill library (from Ch. 6-7)\r\n        self.skills = {\r\n            "navigate": {\r\n                "description": "Navigate to location (x, y, theta)",\r\n                "inputs": ["goal_pose"],\r\n                "outputs": ["status"]\r\n            },\r\n            "detect_object": {\r\n                "description": "Detect object by class name",\r\n                "inputs": ["class_name"],\r\n                "outputs": ["bounding_box"]\r\n            },\r\n            "grasp": {\r\n                "description": "Grasp detected object",\r\n                "inputs": ["object_pose"],\r\n                "outputs": ["grasp_success"]\r\n            }\r\n        }\r\n\r\n    def plan_callback(self, msg):\r\n        command = msg.data\r\n        self.get_logger().info(f"Planning for command: {command}")\r\n\r\n        # Generate plan using LLM\r\n        plan = self.generate_plan(command)\r\n\r\n        # Publish plan\r\n        plan_msg = String()\r\n        plan_msg.data = json.dumps(plan)\r\n        self.publisher.publish(plan_msg)\r\n\r\n    def generate_plan(self, command: str) -> dict:\r\n        """Use GPT-4 to decompose command into skill sequence."""\r\n\r\n        prompt = f"""\r\n        You are a robot task planner. Given a natural language command and available skills,\r\n        generate a step-by-step plan using only the provided skills.\r\n\r\n        Available Skills:\r\n        {json.dumps(self.skills, indent=2)}\r\n\r\n        Command: "{command}"\r\n\r\n        Output JSON format:\r\n        {{\r\n          "steps": [\r\n            {{"skill": "navigate", "params": {{"goal_pose": [x, y, theta]}}}},\r\n            {{"skill": "detect_object", "params": {{"class_name": "cup"}}}}\r\n          ]\r\n        }}\r\n\r\n        Plan:\r\n        """\r\n\r\n        response = openai.ChatCompletion.create(\r\n            model="gpt-4",\r\n            messages=[{"role": "user", "content": prompt}],\r\n            temperature=0.0  # Deterministic\r\n        )\r\n\r\n        plan_text = response[\'choices\'][0][\'message\'][\'content\']\r\n\r\n        # Parse JSON\r\n        plan = json.loads(plan_text)\r\n        return plan\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = LLMPlannerNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"component-3-skill-orchestrator",children:"Component 3: Skill Orchestrator"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="vla_system/skill_orchestrator_node.py"',children:'"""\r\nPurpose: Execute task plan by calling skills sequentially\r\nPrerequisites: Navigation skill, Perception skill\r\nExpected Output: Task execution status\r\n"""\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport json\r\nfrom navigation_skill import NavigationSkill, NavigationStatus\r\nfrom perception_skill import PerceptionSkill\r\n\r\nclass SkillOrchestratorNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'skill_orchestrator\')\r\n\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/task_plan\',\r\n            self.execute_plan_callback,\r\n            10\r\n        )\r\n\r\n        # Initialize skills\r\n        self.nav_skill = NavigationSkill()\r\n        self.perception_skill = PerceptionSkill()\r\n\r\n    def execute_plan_callback(self, msg):\r\n        plan = json.loads(msg.data)\r\n        self.get_logger().info(f"Executing plan: {plan}")\r\n\r\n        for step in plan[\'steps\']:\r\n            skill_name = step[\'skill\']\r\n            params = step[\'params\']\r\n\r\n            self.get_logger().info(f"Step: {skill_name} with {params}")\r\n\r\n            if skill_name == "navigate":\r\n                status = self.nav_skill.navigate_to_pose(params[\'goal_pose\'])\r\n                if status != NavigationStatus.SUCCEEDED:\r\n                    self.get_logger().error("Navigation failed, aborting plan")\r\n                    return\r\n\r\n            elif skill_name == "detect_object":\r\n                detections = self.perception_skill.detect_objects(params[\'class_name\'])\r\n                if len(detections) == 0:\r\n                    self.get_logger().error("Object not detected, aborting plan")\r\n                    return\r\n\r\n            elif skill_name == "grasp":\r\n                success = self.grasp_object(params[\'object_pose\'])\r\n                if not success:\r\n                    self.get_logger().error("Grasp failed, aborting plan")\r\n                    return\r\n\r\n        self.get_logger().info("\u2705 Plan execution complete!")\r\n\r\n    def grasp_object(self, object_pose):\r\n        """Placeholder for grasp skill."""\r\n        self.get_logger().info(f"Grasping object at {object_pose}")\r\n        return True\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = SkillOrchestratorNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"end-to-end-example",children:"End-to-End Example"}),"\n",(0,s.jsx)(n.h3,{id:"user-command-go-to-the-kitchen-and-find-a-cup",children:'User Command: "Go to the kitchen and find a cup"'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 1: Voice \u2192 Text"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:'Speech Recognition: "Go to the kitchen and find a cup"\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 2: Text \u2192 Plan (LLM)"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\r\n  "steps": [\r\n    {"skill": "navigate", "params": {"goal_pose": [5.0, 2.0, 0.0]}},\r\n    {"skill": "detect_object", "params": {"class_name": "cup"}}\r\n  ]\r\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Step 3: Execute Plan"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"1. Navigate to kitchen (5.0, 2.0) \u2192 SUCCEEDED\r\n2. Detect cup \u2192 Found at (5.2, 2.1, 0.8)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Task complete!"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-integrate-whisper-speech-recognition-easy",children:"Exercise 1: Integrate Whisper Speech Recognition (Easy)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Install ",(0,s.jsx)(n.code,{children:"openai-whisper"})]}),"\n",(0,s.jsx)(n.li,{children:"Record 3-second audio clips"}),"\n",(0,s.jsxs)(n.li,{children:["Transcribe and publish to ",(0,s.jsx)(n.code,{children:"/voice_command"})]}),"\n",(0,s.jsx)(n.li,{children:'Test with command: "Navigate to the door"'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Transcription accuracy \u226590% for clear speech"}),"\n",(0,s.jsxs)(n.li,{children:["Latency ",(0,s.jsx)(n.code,{children:"<2s"})," from speech end to text output"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-build-llm-planner-medium",children:"Exercise 2: Build LLM Planner (Medium)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Create prompt template for skill library"}),"\n",(0,s.jsx)(n.li,{children:"Query GPT-4 API with user command"}),"\n",(0,s.jsx)(n.li,{children:"Parse JSON plan output"}),"\n",(0,s.jsx)(n.li,{children:"Validate plan (all skills exist, params valid)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Valid plan generated for \u22655 test commands"}),"\n",(0,s.jsx)(n.li,{children:"Plan uses only available skills"}),"\n",(0,s.jsx)(n.li,{children:"Failure handling (invalid skill \u2192 return error)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-end-to-end-vla-pipeline-hard",children:"Exercise 3: End-to-End VLA Pipeline (Hard)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate all 3 components (speech, LLM, orchestrator)"}),"\n",(0,s.jsx)(n.li,{children:"Execute \u22653 multi-step tasks"}),"\n",(0,s.jsx)(n.li,{children:"Handle failures gracefully (skill failure \u2192 re-plan)"}),"\n",(0,s.jsx)(n.li,{children:"Measure success rate"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Success rate \u226570% for test suite"}),"\n",(0,s.jsxs)(n.li,{children:["Average task completion time ",(0,s.jsx)(n.code,{children:"<2min"})]}),"\n",(0,s.jsx)(n.li,{children:"Error recovery implemented (\u22651 retry per skill)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"spec-driven-development-",children:"Spec-Driven Development \u2b50"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Layer 4 Requirement"}),": Formalize task specifications."]}),"\n",(0,s.jsx)(n.h3,{id:"task-specification-format",children:"Task Specification Format"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'task:\r\n  id: fetch_cup_001\r\n  description: "Navigate to kitchen and retrieve cup"\r\n\r\n  preconditions:\r\n    - robot_localized: true\r\n    - battery_level: ">20%"\r\n\r\n  steps:\r\n    - skill: navigate\r\n      params:\r\n        goal: kitchen\r\n      postconditions:\r\n        - at_location: kitchen\r\n\r\n    - skill: detect_object\r\n      params:\r\n        class: cup\r\n      postconditions:\r\n        - cup_detected: true\r\n\r\n    - skill: grasp\r\n      params:\r\n        object: cup\r\n      postconditions:\r\n        - holding: cup\r\n\r\n  success_criteria:\r\n    - holding: cup\r\n    - at_location: kitchen\r\n\r\n  failure_recovery:\r\n    - on: navigation_failed\r\n      action: retry\r\n      max_attempts: 3\r\n\r\n    - on: cup_not_detected\r\n      action: re_plan\r\n      alternate_skill: search_area\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(r,{children:[(0,s.jsxs)("summary",{children:[(0,s.jsx)("strong",{children:"Q1"}),": Why use LLMs for task planning instead of hard-coded scripts?"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Answer"}),":"]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hard-coded"}),": Fast, deterministic, but brittle. Requires manual updates for new tasks."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM"}),": Flexible, generalizes to new commands, handles ambiguity."]}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Trade-off"}),": LLMs add latency (~1-2s) but enable natural language interaction."]})]}),"\n",(0,s.jsxs)(r,{children:[(0,s.jsxs)("summary",{children:[(0,s.jsx)("strong",{children:"Q2"}),": How do you handle LLM hallucinations (invalid skills)?"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Answer"}),":"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Validate plan against skill library"}),"\n",(0,s.jsx)(n.li,{children:"Reject plans with unknown skills"}),"\n",(0,s.jsx)(n.li,{children:"Use function calling (GPT-4) to constrain outputs"}),"\n",(0,s.jsx)(n.li,{children:"Implement retry with clearer prompt"}),"\n"]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"self-check",children:"Self-Check"}),"\n",(0,s.jsx)(n.p,{children:"Before Chapter 9, verify you can:"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate speech recognition with Whisper"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use LLM (GPT-4) for task decomposition"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Orchestrate skills from Ch. 6-7"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Build end-to-end VLA pipeline"})," \u2b50"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Write formal task specifications (YAML)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement failure recovery (retry, re-plan)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.admonition,{title:"What's Next?",type:"note",children:(0,s.jsxs)(n.p,{children:["Continue to ",(0,s.jsx)(n.a,{href:"./chapter-9-capstone",children:"Chapter 9: Capstone Project"})," to build a complete autonomous humanoid system integrating all skills."]})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/gpt",children:"GPT-4 API Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"VLA Paper (Driess et al., 2023)"})}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Chapter Status"}),": \u2705 Complete\r\n",(0,s.jsx)(n.strong,{children:"Last Updated"}),": 2025-11-29\r\n",(0,s.jsx)(n.strong,{children:"Layer"}),": 4 (Spec-Driven Integration)"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>l,x:()=>a});var i=r(6540);const s={},t=i.createContext(s);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);